---
title: "Report"
author: "Leonard Maaya"
date: "2023-07-06"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(tidyverse)
library(ggplot2)

system("conda env list")
```

# R
## Iris dataset

The iris dataset has `r nrow(iris)` observations and `r NCOL(iris)` columns/variables/features.


# Python

* Python for absolute beginners:
  * <https://www.youtube.com/watch?v=LHRn9NMNBCQ&list=PLbvhRHYrmshRFWUrS6x2LgeE4CMte_m5K>
  
```{python fibonacci, warning  = FALSE, message = FALSE, echo = FALSE}
# Python program to generate Fibonacci series until 'n' value

a, b = 0, 1
nums = []
count = 1

# print("Fibonacci Series: ", end = " ")
while(count <= 20):
  nums.append(b)
  count += 1
  a, b = b, a + b

n = len(nums)  
```

## Fibonacci series

The first `r py$n` terms in a Fibonacci series are: `r py$nums`.

# Data science basics - in Python

* check here: 
<https://www.youtube.com/watch?v=JL_grPUnXzY&list=PLeo1K3hjS3us_ELKYSj_Fth2tIEkdKXvV> 

## Lasso (L1 regularization) and Ridge (L2) regression

```{python L1L2regularization, warning  = FALSE, message = FALSE, echo = FALSE, results=FALSE}

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns


import warnings
warnings.filterwarnings('ignore')

# Import Melbourne housing data

dataset = pd.read_csv('C:/Users/Public/lmaaya/data/Melbourne_housing_FULL.csv')
dataset = dataset.iloc[:1000] # select a few rows for demonstration purposes

# dataset.head()
# dataset.nunique()
# dataset.shape

# check and resolve for NA values
# dataset.isna().sum()

cols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']

dataset[cols_to_fill_zero] = dataset[cols_to_fill_zero].fillna(0)
# dataset.isna().sum()

dataset['Landsize'] = dataset['Landsize'].fillna(dataset.Landsize.mean())
dataset['BuildingArea'] = dataset['BuildingArea'].fillna(dataset.BuildingArea.mean())
# dataset.isna().sum()

dataset.dropna(inplace=True)
# dataset.isna().sum()

# Convert categoricals to dummies
dataset = pd.get_dummies(dataset, drop_first=True)
# dataset.head()

x = dataset.drop('Price', axis = 1)
y = dataset['Price']


# Train and test datasets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=2)

# Train the model
from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X_train, Y_train)

score = []
score.append(round(reg.score(X_train, Y_train), 2))
score.append(round(reg.score(X_test, Y_test),2))

# Lasso regression//L1 regularization

from sklearn import linear_model
scoreL1 = []
lasso_reg = linear_model.Lasso(alpha = 50, max_iter = 1000, tol = 0.1)
lasso_reg.fit(X_train, Y_train)

scoreL1.append(round(reg.score(X_train, Y_train), 2))
scoreL1.append(round(reg.score(X_test, Y_test),2))

# Ridge regression // L2 regularization
from sklearn.linear_model import Ridge
scoreL2 = []
ridge_reg = Ridge(alpha = 50, max_iter = 1000, tol = 0.1)
ridge_reg.fit(X_train, Y_train)

scoreL2.append(round(ridge_reg.score(X_train, Y_train), 2))
scoreL2.append(round(ridge_reg.score(X_test, Y_test),2))
```


The score for the model without regularization on the training data is `r py$score[1]` while it is `r py$score[2]` on the test data. Elsewhere, the score for the model with L1 regularization - i.e. Lasso regression - on training (test) data is `r py$scoreL1[1]` (`r py$scoreL1[2]`). For L2 regularization - Ridge regression - the score is `r py$scoreL2[1]` (`r py$scoreL2[2]`). These results show that regularization does improve the predictiveness of trained models.

```{python, warning=FALSE, message = FALSE, echo = FALSE}


x = [i + 2*pow(i, 2) + 2*pow(i, 3) for i in range(1, 10, 1) if i%2== 1]
[print(x, "*", y, "= ", x*y) for x in range(1, 10, 2) for y in range(1, 10, 3) if x==y]

for x in range(1, 10, 2):
  for y in range(1, 10, 3):
    if x == y:
      continue
    print(x, "*", y, "= ", x*y)

```


```{python}

#import StatsBombPy
```
